{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmuller/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py:115: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/tmuller/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py:123: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/tmuller/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py:125: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0 : Lx1: 54.880035 Lx2: 59.166199 Lzpc: 160.260696 Lzgc: 6.763613 Lzs : 0.321085\n"
     ]
    }
   ],
   "source": [
    "# %load draw.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials import mnist\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"\", \"\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "## MODEL PARAMETERS ## \n",
    "n_input=600\n",
    "img_size=n_input #B*A # the canvas size\n",
    "z_pc_size=50 #QSampler output size\n",
    "z_gc_size=10\n",
    "train_iters=3000 # 10000\n",
    "learning_rate=1e-3 # 1e-3 # learning rate for optimizer\n",
    "eps=1e-8 # epsilon for numerical stability\n",
    "state_size=16\n",
    "batch_size=20 # training minibatch size\n",
    "beta=2\n",
    "head_dir_on=1\n",
    "\n",
    "def seq_sort(it_num,b_size,data):\n",
    "    n_data=data.shape[0]\n",
    "    n_input=data.shape[1]\n",
    "    seq_len=int(n_data/b_size)\n",
    "    data_sort=np.zeros((b_size,n_input))\n",
    "    a=0\n",
    "    for i in range(n_data):\n",
    "        if np.mod(i,seq_len)==it_num:            \n",
    "            data_sort[a,:]=data[i,:]\n",
    "            a=a+1\n",
    "    return data_sort\n",
    "\n",
    "def accuracy(states,reconstuction,inp):\n",
    "    ls_prediction=np.zeros((batch_size,state_size))\n",
    "    prediction=np.zeros((batch_size,1))\n",
    "    for j in range(batch_size):\n",
    "        pc_out=reconstuction[j,:]\n",
    "        for k in range(state_size):\n",
    "            ls_prediction[j,k]=np.sum((pc_out-inp[k])**2)\n",
    "        prediction[j]=np.where(ls_prediction[j,:]==np.min(ls_prediction[j,:]))\n",
    "    temp=np.shape(np.where((prediction-states)==0))\n",
    "    return (temp[1]/batch_size)*100\n",
    "\n",
    "def linear_track(length):    \n",
    "    A_both=np.zeros((length,length))\n",
    "    A_for=np.zeros((length,length))\n",
    "    A_back=np.zeros((length,length))\n",
    "    for i in range(length):\n",
    "        A_both[i,np.mod(i+1,length)]=1           \n",
    "        A_both[np.mod(i+1,length),i]=1\n",
    "        A_for[i,np.mod(i+1,length)]=1           \n",
    "        A_back[np.mod(i+1,length),i]=1\n",
    "    return A_both,A_for,A_back\n",
    "\n",
    "def square_box(states):\n",
    "    width=int(np.sqrt(states))\n",
    "    A_box=np.zeros((states,states))\n",
    "    #state number counts down and then across\n",
    "    for i in range(states):\n",
    "        if i+width<states: #right - left\n",
    "            A_box[i,i+width]=1\n",
    "            A_box[i+width,i]=1\n",
    "        \n",
    "        if np.mod(i,width)!=0: #up - down\n",
    "            A_box[i,i-1]=1\n",
    "            A_box[i-1,i]=1            \n",
    "    return A_box\n",
    "\n",
    "def square_torus(states):\n",
    "    width=int(np.sqrt(states))\n",
    "    A_box=np.zeros((states,states))\n",
    "    #state number counts down and then across\n",
    "    for i in range(states):\n",
    "        if i+width<states: #right - left\n",
    "            A_box[i,i+width]=1\n",
    "            A_box[i+width,i]=1\n",
    "        else:\n",
    "            A_box[i,i-(width-1)*width]=1\n",
    "            A_box[i-(width-1)*width,i]=1        \n",
    "            \n",
    "        if np.mod(i,width)!=0: #up - down\n",
    "            A_box[i,i-1]=1\n",
    "            A_box[i-1,i]=1\n",
    "        else:\n",
    "            A_box[i,i+width-1]=1\n",
    "            A_box[i+width-1,i]=1         \n",
    "    return A_box\n",
    "\n",
    "def input_cells(state_size,n_input):\n",
    "    inp=np.zeros((state_size,n_input))\n",
    "    for i in range(state_size):\n",
    "        #inp[i,:]=np.random.uniform(0,1,(1,n_input))*np.random.uniform(0,1,(1,n_input))\n",
    "        inp[i,:]=np.random.uniform(0,1,(1,n_input))\n",
    "    return inp    \n",
    "\n",
    "def walking_data(state_size,time_steps,n_input,inp):\n",
    "    #A_both,A_1,A_2=linear_track(state_size)\n",
    "    #A_1=square_box(state_size)\n",
    "    A_1=square_torus(state_size)\n",
    "    data_t=np.zeros((time_steps,n_input))\n",
    "    data_plus=np.zeros((time_steps,n_input))\n",
    "    context=np.zeros((time_steps,1))\n",
    "    state=np.zeros((time_steps+1,1))\n",
    "    head_dir=np.zeros((time_steps,4))\n",
    "    width=int(np.sqrt(state_size))\n",
    "    #start at random state\n",
    "    #have two difference contexts\n",
    "    state[0,0]=np.random.choice(range(state_size),1)\n",
    "    for i in range(time_steps):\n",
    "        data_t[i,:]=inp[state[i,0],:]\n",
    "        q=np.mod(i,batch_size) # get sequences of contexts        \n",
    "        if q<batch_size//2: #np.random.rand(1) <= 0.5: # \n",
    "            Adj=A_1\n",
    "            context[i]=0;\n",
    "        else:\n",
    "            Adj=A_1\n",
    "            context[i]=1; #0\n",
    "        available=np.where(Adj[state[i,0],:]==1)[0]\n",
    "        state[i+1,0]=np.random.choice(available,1)\n",
    "        data_plus[i,:]=inp[state[i+1,0],:]\n",
    "        # consider square environment. if move +1=up, -1=down, +length=right, -length=left.\n",
    "        if head_dir_on==1:\n",
    "            diff=state[i+1,0]-state[i,0]\n",
    "            if diff==1 or diff==-(width-1):  #down\n",
    "                head_dir[i,0]=1 \n",
    "            elif diff==-1 or diff==(width-1):  #up\n",
    "                head_dir[i,1]=1\n",
    "            elif diff==width or diff==-width*(width-1):  #right\n",
    "                head_dir[i,2]=1\n",
    "            elif diff==-width or diff==width*(width-1):  #left\n",
    "                head_dir[i,3]=1\n",
    "            \n",
    "    return (context,data_t,data_plus,state,head_dir)\n",
    "\n",
    "## BUILD MODEL ## \n",
    "DO_SHARE=None # workaround for variable_scope(reuse=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(batch_size,img_size)) # input (batch_size * img_size)\n",
    "gc_t = tf.placeholder(tf.float32,shape=(batch_size,z_gc_size)) # input (batch_size * img_size)\n",
    "mumu = tf.placeholder(tf.float32,shape=(batch_size,z_pc_size)) # input (batch_size * img_size)\n",
    "logsigsig = tf.placeholder(tf.float32,shape=(batch_size,z_pc_size)) # input (batch_size * img_size)\n",
    "context = tf.placeholder(tf.float32,shape=(batch_size,1)) # input (batch_size * img_size)\n",
    "head_d = tf.placeholder(tf.float32,shape=(batch_size,4)) # input (batch_size * img_size)\n",
    "x_n = tf.placeholder(tf.float32,shape=(batch_size,img_size)) # input (batch_size * img_size)\n",
    "e_pc_1=tf.random_normal((batch_size,z_pc_size), mean=0, stddev=1) # Qsampler noise\n",
    "e_pc_2=tf.random_normal((batch_size,z_pc_size), mean=0, stddev=1) # Qsampler noise\n",
    "e_gc=tf.random_normal((batch_size,z_gc_size), mean=0, stddev=1) # Qsampler noise\n",
    "train_iteration=tf.placeholder(tf.int32,shape=())\n",
    "\n",
    "def linear(x,output_dim):\n",
    "    w=tf.get_variable(\"w\", [x.get_shape()[1], output_dim]) \n",
    "    b=tf.get_variable(\"b\", [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.matmul(x,w)+b\n",
    " \n",
    "## PLACE CELL STUFF\n",
    "def encode_PC(x):    \n",
    "    with tf.variable_scope(\"encoder_pc\",reuse=DO_SHARE):\n",
    "        h=linear(x,z_pc_size)        \n",
    "    return tf.tanh(h)  \n",
    "\n",
    "def sampleQ_PC(h_enc_x):\n",
    "    with tf.variable_scope(\"mu_x2pc\",reuse=DO_SHARE):\n",
    "        mu=linear(h_enc_x,z_pc_size)\n",
    "    with tf.variable_scope(\"sigma_x2pc\",reuse=DO_SHARE):\n",
    "        logsigma=linear(h_enc_x,z_pc_size)\n",
    "        sigma=tf.exp(logsigma)\n",
    "    return (mu + sigma*e_pc_1, mu, logsigma, sigma)\n",
    "\n",
    "def sampleQ_PC_1(h_enc_x,h_dec_gc): \n",
    "    _, mu_gc2pc,logsigma_gc2pc,sigma_gc2pc=sampleQ_PC_2(h_dec_gc)    \n",
    "    _, mu_x2pc,logsigma_x2pc,sigma_x2pc=sampleQ_PC(h_enc_x)    \n",
    "    \n",
    "    logsigma=-0.5*tf.log(1/tf.square(sigma_gc2pc) + 1/tf.square(sigma_x2pc))\n",
    "    sigma=tf.exp(logsigma) \n",
    "    mu=sigma*(mu_gc2pc/tf.square(sigma_gc2pc) + mu_x2pc/tf.square(sigma_x2pc))\n",
    "    return (mu + sigma*e_pc_1, mu, logsigma, sigma)\n",
    "\n",
    "def sampleQ_PC_2(h_enc):    \n",
    "    with tf.variable_scope(\"mu_gc2pc\",reuse=DO_SHARE):\n",
    "        mu=linear(h_enc,z_pc_size)\n",
    "    with tf.variable_scope(\"sigma_gc2pc\",reuse=DO_SHARE):\n",
    "        logsigma=linear(h_enc,z_pc_size)\n",
    "        sigma=tf.exp(logsigma)    \n",
    "    return (mu + sigma*e_pc_2, mu, logsigma, sigma)\n",
    "\n",
    "def decode_PC(x):\n",
    "    with tf.variable_scope(\"decoder_pc\",reuse=DO_SHARE):\n",
    "        h=linear(x,img_size)\n",
    "    return tf.tanh(h) \n",
    "    \n",
    "def write_PC(h_dec):\n",
    "    with tf.variable_scope(\"write_pc\",reuse=DO_SHARE):\n",
    "        return linear(h_dec,img_size)    \n",
    "\n",
    "## GRID CELL STUFF\n",
    "def encode_GC(x):\n",
    "    mult=1\n",
    "    with tf.variable_scope(\"encoder_gc\",reuse=DO_SHARE):\n",
    "        h=linear(x,mult*z_gc_size)\n",
    "    #xp=tf.tanh(h)    \n",
    "    #with tf.variable_scope(\"encoder_gc_2\",reuse=DO_SHARE):\n",
    "    #    h=linear(xp,z_gc_size)\n",
    "    return tf.tanh(h)           \n",
    "\n",
    "def sampleQ_GC(h_enc):    \n",
    "    with tf.variable_scope(\"mu_gc\",reuse=DO_SHARE):\n",
    "        mu=linear(h_enc,z_gc_size)\n",
    "    with tf.variable_scope(\"sigma_gc\",reuse=DO_SHARE):\n",
    "        logsigma=linear(h_enc,z_gc_size)\n",
    "        sigma=tf.exp(logsigma)\n",
    "    return (mu + 0*sigma*e_gc, mu, logsigma, sigma)\n",
    "\n",
    "def decode_GC(x):\n",
    "    with tf.variable_scope(\"decoder_gc\",reuse=DO_SHARE):\n",
    "        h=linear(x,z_pc_size)\n",
    "    return tf.tanh(h) \n",
    "    \n",
    "def write_GC(h_dec):\n",
    "    with tf.variable_scope(\"write_gc\",reuse=DO_SHARE):\n",
    "        return linear(h_dec,z_pc_size)    \n",
    "\n",
    "## STATE VARIABLES ## \n",
    "mus_pc,logsigmas_pc,sigmas_pc,zs_pc=[0]*2,[0]*2,[0]*2,[0]*2 \n",
    "mus_gc,logsigmas_gc,sigmas_gc,zs_gc=[0],[0],[0],[0] \n",
    "\n",
    "\"\"\"\n",
    "## COMPUTATIONAL GRAPH\n",
    "#place cell t=1\n",
    "h_pc1_enc=encode_PC(x)\n",
    "h_dec_gc_t=decode_GC(gc_t)\n",
    "zs_pc[0],mus_pc[0],logsigmas_pc[0],sigmas_pc[0]=sampleQ_PC_1(h_pc1_enc,h_dec_gc_t)\n",
    "h_dec_pc=decode_PC(zs_pc[0])\n",
    "PC1_rec=write_PC(h_dec_pc)\n",
    "PC1_recons=tf.nn.sigmoid(PC1_rec)\n",
    "#grid cell\n",
    "h_gc_enc=encode_GC(tf.concat(1,[context,head_d,mus_pc[0]]))\n",
    "z_gc,mu_gc,logsigma_gc,sigma_gc=sampleQ_GC(h_gc_enc)\n",
    "DO_SHARE=True\n",
    "h_dec_gc=decode_GC(z_gc)\n",
    "#place cell t=2\n",
    "zs_pc[1],mus_pc[1],logsigmas_pc[1],sigmas_pc[1]=sampleQ_PC_2(h_dec_gc)\n",
    "#DO_SHARE=True\n",
    "h_dec_pc=decode_PC(zs_pc[1])\n",
    "PC2_rec=write_PC(h_dec_pc)\n",
    "PC2_recons=tf.nn.sigmoid(PC2_rec)\n",
    "# z_pc_t+1\n",
    "h_pc1_enc_2=encode_PC(x_n)\n",
    "zs_pc_2,mus_pc_2,logsigmas_pc_2,_=sampleQ_PC_1(h_pc1_enc_2,h_dec_gc)\n",
    "\"\"\"\n",
    "\n",
    "## COMPUTATIONAL GRAPH -- this is where the model is specified, calls above encoders, decoders, samplers, etc\n",
    "#place cell t=1\n",
    "h_pc1_enc=encode_PC(x)\n",
    "zs_pc[0],mus_pc[0],logsigmas_pc[0],sigmas_pc[0]=sampleQ_PC(h_pc1_enc)\n",
    "h_dec_pc=decode_PC(zs_pc[0])\n",
    "PC1_rec=write_PC(h_dec_pc)\n",
    "PC1_recons=tf.nn.sigmoid(PC1_rec)\n",
    "#grid cell\n",
    "h_gc_enc=encode_GC(tf.concat(1,[context,head_d,h_pc1_enc]))\n",
    "z_gc,mu_gc,logsigma_gc,sigma_gc=sampleQ_GC(h_gc_enc)\n",
    "h_dec_gc=decode_GC(z_gc)\n",
    "DO_SHARE=True\n",
    "#place cell t=2\n",
    "zs_pc[1],mus_pc[1],logsigmas_pc[1],sigmas_pc[1]=sampleQ_PC(h_dec_gc)\n",
    "h_dec_pc=decode_PC(zs_pc[1])\n",
    "PC2_rec=write_PC(h_dec_pc)\n",
    "PC2_recons=tf.nn.sigmoid(PC2_rec)\n",
    "\n",
    "# z_pc_t+1\n",
    "h_pc1_enc_t_plus=encode_PC(x_n)\n",
    "zs_pc_t_plus,mus_pc_t_plus,logsigmas_pc_t_plus,sigmas_pc_t_plus=sampleQ_PC(h_pc1_enc_t_plus)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## LOSS FUNCTION ## ~\n",
    "def binary_crossentropy(t,o):\n",
    "    return -(t*tf.log(o+eps) + (1.0-t)*tf.log(1.0-o+eps))\n",
    "\n",
    "def squared_error(t,o):\n",
    "    return tf.square(t-o)\n",
    "\n",
    "## RECONSTRUCTIONS ERRORS\n",
    "Lx1=tf.reduce_sum(squared_error(x,PC1_recons),1) # reconstruction term\n",
    "Lx1=tf.reduce_mean(Lx1)\n",
    "Lx2=tf.reduce_sum(squared_error(x_n,PC2_recons),1) # reconstruction term\n",
    "Lx2=tf.reduce_mean(Lx2)\n",
    "## KL PLACE CELLS\n",
    "kl_terms_pc=[0]*2\n",
    "for t in range(2):\n",
    "    mu2_pc=tf.square(mus_pc[t])\n",
    "    sigma2_pc=tf.square(sigmas_pc[t])\n",
    "    logsigma_pc=logsigmas_pc[t]\n",
    "    c=mus_pc[t]/(2*sigmas_pc[t])\n",
    "    c_sq=tf.square(c)\n",
    "    kl_terms_pc[t]=tf.reduce_sum(-logsigma_pc+beta*(tf.sqrt(2*sigma2_pc/np.pi)*tf.exp(-c_sq)+\\\n",
    "                    mus_pc[t]*tf.erf(c)),1)-.5-.5*tf.log(2*np.pi)-tf.log(2/beta)# each kl term is (1xminibatch)\n",
    "KL_pc=tf.add_n(kl_terms_pc) # this is 1xminibatch, corresponding to summing kl_terms from 1:T\n",
    "#KL_pc=kl_terms_pc[0]\n",
    "Lz_pc=tf.reduce_mean(KL_pc) # average over minibatches\n",
    "\n",
    "## KL GRID CELLS\n",
    "mu2_gc=tf.square(mu_gc)\n",
    "sigma2_gc=tf.square(sigma_gc)\n",
    "kl_terms_gc=0.5*tf.reduce_sum(mu2_gc+sigma2_gc-2*logsigma_gc,1)- .5 # each kl term is (1xminibatch)\n",
    "#KL_gc=tf.add_n(kl_terms_gc)\n",
    "Lz_gc=tf.reduce_mean(kl_terms_gc) # average over minibatches\n",
    "\n",
    "#KL beetween zs\n",
    "mu2_pc=tf.square(mus_pc[1])\n",
    "sigma2_pc=tf.square(sigmas_pc[1])\n",
    "logsigma_pc=logsigmas_pc[1]\n",
    "\"\"\"\n",
    "#ATTEMPT 1 - treat mu,sigma at t+1 as constant - maybe doesnt work as get differnt z's due to stochasticity?\n",
    "sigsig=tf.exp(logsigsig)\n",
    "sigsig2=tf.square(sigsig)\n",
    "mu2=tf.square(mumu)\n",
    "KL_between_zs=0.5*tf.reduce_sum(2*logsigsig-2*logsigma_pc+\\\n",
    "                                (mu2+mu2_pc+sigma2_pc-2*mus_pc[1]*mumu)/sigsig2,1)- .5 \n",
    "Lz_zs=tf.reduce_mean(KL_between_zs)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#ATTEMPT 2 - treat mu,sigma at t+1 as variables\n",
    "sigsig=tf.exp(logsigmas_pc_2)\n",
    "sigsig2=tf.square(sigsig)\n",
    "mu2=tf.square(mus_pc_2)\n",
    "KL_between_zs=0.5*tf.reduce_sum(2*logsigmas_pc_2-2*logsigma_pc+\\\n",
    "                                (mu2+mu2_pc+sigma2_pc-2*mus_pc[1]*mus_pc_2)/sigsig2,1)- .5 \n",
    "Lz_zs=tf.reduce_mean(KL_between_zs)\n",
    "\"\"\"\n",
    "\n",
    "Lz_zs=tf.reduce_mean(squared_error(zs_pc[1],zs_pc_t_plus))\n",
    "\n",
    "## COST\n",
    "#cost_all=Lx1+Lx2+Lz_pc+Lz_gc+Lz_zs\n",
    "#cost_all=Lx2#Lx1+Lx2#+Lz_pc#+Lz_gc+Lz_zs\n",
    "cost_all=Lz_zs+Lz_gc\n",
    "####cost=Lx1+Lz_pc+Lz_zs+Lz_gc#+Lx2\n",
    "cost_autoenc=Lx1+tf.reduce_mean(kl_terms_pc[0])\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"encoder_gc\",reuse=True):\n",
    "    a=tf.get_variable(\"w\")\n",
    "    b=tf.get_variable(\"b\")\n",
    "with tf.variable_scope(\"mu_gc\",reuse=True):\n",
    "    a1=tf.get_variable(\"w\")\n",
    "    b1=tf.get_variable(\"b\") \n",
    "with tf.variable_scope(\"sigma_gc\",reuse=True):\n",
    "    a2=tf.get_variable(\"w\")\n",
    "    b2=tf.get_variable(\"b\") \n",
    "with tf.variable_scope(\"decoder_gc\",reuse=True):\n",
    "    a3=tf.get_variable(\"w\")\n",
    "    b3=tf.get_variable(\"b\")\n",
    "\"\"\"\n",
    "with tf.variable_scope(\"mu_gc2pc\",reuse=True):\n",
    "    a4=tf.get_variable(\"w\")\n",
    "    b4=tf.get_variable(\"b\") \n",
    "with tf.variable_scope(\"sigma_gc2pc\",reuse=True):\n",
    "    a5=tf.get_variable(\"w\")\n",
    "    b5=tf.get_variable(\"b\") \n",
    "#with tf.variable_scope(\"write_gc\",reuse=True):\n",
    "   # a6=tf.get_variable(\"w\")\n",
    "    #b6=tf.get_variable(\"b\") \n",
    "#tf.Variable(a, trainable=False)\n",
    "\"\"\"\n",
    "#my_var_list={a,b,a1,b1,a2,b2,a3,b3,a4,b4,a5,b5}#,a6,b6} \n",
    "my_var_list={a,b,a1,b1,a2,b2,a3,b3}#,a4,b4,a5,b5}#,a6,b6} \n",
    "    \n",
    "    \n",
    "#tf.Variable([tf.get_variable(\"decoder_pc/w\")], trainable=False)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "#grads=optimizer.compute_gradients(cost)\n",
    "if tf.assert_less(train_iteration,500):\n",
    "    grads=optimizer.compute_gradients(cost_autoenc)\n",
    "    #grads=optimizer.compute_gradients(cost_all,var_list=my_var_list)\n",
    "else:\n",
    "    grads=optimizer.compute_gradients(cost_all,var_list=my_var_list)\n",
    "    #grads=optimizer.compute_gradients(cost_autoenc)\n",
    "    \n",
    "    \n",
    "for i,(g,v) in enumerate(grads):\n",
    "    if g is not None:\n",
    "        grads[i]=(tf.clip_by_norm(g,5),v) # clip gradients\n",
    "train_op=optimizer.apply_gradients(grads)\n",
    "\n",
    "\n",
    "## RUN TRAINING ## \n",
    "fetches=[]\n",
    "#fetches.extend([Lx1,Lx2,Lz_pc,Lz_gc,Lz_zs,z_gc,train_op])\n",
    "fetches.extend([Lx1,Lx2,Lz_pc,Lz_gc,Lz_zs,z_gc,train_op])\n",
    "Lx1s=[0]*train_iters\n",
    "Lx2s=[0]*train_iters\n",
    "Lz_pcs=[0]*train_iters\n",
    "Lz_gcs=[0]*train_iters\n",
    "Lzs=[0]*train_iters\n",
    "\n",
    "#fetches2=[]\n",
    "#fetches2.extend([mus_pc_2,logsigmas_pc_2])\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "saver = tf.train.Saver() # saves variables learned during training\n",
    "tf.initialize_all_variables().run()\n",
    "#saver.restore(sess, \"infer_place_grid_v1.ckpt\") # to restore from model, uncomment this line\n",
    "inp_1=input_cells(state_size,n_input)\n",
    "inp_2=input_cells(state_size,n_input)\n",
    "seq_len=10\n",
    "for i in range(train_iters):\n",
    "    if np.random.rand(1) <= 0.5:\n",
    "        inp=inp_1\n",
    "    else:\n",
    "        inp=inp_1\n",
    "        \n",
    "    gcs=np.zeros((batch_size,z_gc_size))\n",
    "    \n",
    "    (contexxy,data_t,data_plus,states,head_dir)=walking_data(state_size,seq_len*batch_size,n_input,inp)\n",
    "    \n",
    "    for ii in range(seq_len):\n",
    "        \n",
    "        x_t=seq_sort(ii,batch_size,data_t)\n",
    "        x_p=seq_sort(ii,batch_size,data_plus)\n",
    "        cx=seq_sort(ii,batch_size,contexxy)\n",
    "        hd=seq_sort(ii,batch_size,head_dir) \n",
    "        \n",
    "        feed_dict={x:x_t,x_n:x_p,context:cx,head_d:hd,gc_t:gcs,train_iteration:i}\n",
    "    \n",
    "        #results=sess.run([mus_pc_2,logsigmas_pc_2],feed_dict)\n",
    "        #results=sess.run(fetches2,feed_dict)\n",
    "        #mu,logsig=results    \n",
    "    \n",
    "        #feed_dict={x:x_t,x_n:x_p,context:cx,head_d:hd,gc_t:gcs,mumu:mu,logsigsig:logsig}\n",
    "    \n",
    "        results=sess.run(fetches,feed_dict)\n",
    "        Lx1s[i],Lx2s[i],Lz_pcs[i],Lz_gcs[i],Lzs[i],gcs,_=results\n",
    "    \n",
    "        #gcs=np.zeros((batch_size,z_gc_size))\n",
    "\n",
    "    if i%100==0:\n",
    "        print(\"iter=%d : Lx1: %f Lx2: %f Lzpc: %f Lzgc: %f Lzs : %f\" % (i*seq_len,Lx1s[i],Lx2s[i],Lz_pcs[i],Lz_gcs[i],Lzs[i]))\n",
    "        #print(\"iter=%d : Lx1: %f Lx2: %f\" % (i*seq_len,Lx1s[i],Lx2s[i]))#,Lz_pcs[i],Lz_gcs[i],Lzs[i]))\n",
    "\n",
    "## TRAINING FINISHED ## \n",
    "\n",
    "state1=seq_sort(ii,batch_size,states[0:-1])\n",
    "state2=seq_sort(ii,batch_size,states[1:])\n",
    "\n",
    "PC1_reconsructions=sess.run(PC1_recons,feed_dict)\n",
    "PC1_reconsructions=np.array(PC1_reconsructions)\n",
    "PC2_reconsructions=sess.run(PC2_recons,feed_dict)\n",
    "PC2_reconsructions=np.array(PC2_reconsructions)\n",
    "## test accuracy\n",
    "correct1=accuracy(state1,PC1_reconsructions,inp)\n",
    "correct2=accuracy(state2,PC2_reconsructions,inp)\n",
    "\n",
    "place_cells=sess.run(zs_pc,feed_dict)\n",
    "place_cells_mus=sess.run(mus_pc,feed_dict)\n",
    "grid_cells=sess.run(z_gc,feed_dict)\n",
    "grid_cells_mus=sess.run(mu_gc,feed_dict)\n",
    "out_file=os.path.join(FLAGS.data_dir,\"infer_place_grid_v1_data.npy\")\n",
    "np.save(out_file,[PC1_reconsructions,PC2_reconsructions,Lx1s,Lx2s,Lz_pcs,Lz_gcs,contexxy,inp_1,inp_2])\n",
    "print(\"Outputs saved in file: %s\" % out_file)\n",
    "\n",
    "ckpt_file=os.path.join(FLAGS.data_dir,\"infer_place_grid_v1.ckpt\")\n",
    "print(\"Model saved in file: %s\" % saver.save(sess,ckpt_file))\n",
    "\n",
    "print(\"PC1 accuracy=%d\" % correct1)\n",
    "print(\"PC2 accuracy=%d\" % correct2)\n",
    "\n",
    "sess.close()\n",
    "\n",
    "print('Done drawing! Have a nice day! :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#out_file='simple_model_adj_data.npy'\n",
    "#[PC1,PC2,Lx1s,Lx2s,Lz_pcs,Lz_gcs,contexxy]=np.load(out_file)\n",
    "#batch_size,img_size=PC2.shape\n",
    "plt.plot(Lx1s,label='Reconstruction Loss Lx : PC1')\n",
    "plt.plot(Lx2s,label='Reconstruction Loss Lx : PC2')    \n",
    "plt.plot(np.add(Lz_pcs,Lz_gcs),label='Latent Loss Lz')\n",
    "plt.xlabel('iterations')\n",
    "plt.legend()\n",
    "\n",
    "tt=np.array(place_cells_mus)\n",
    "img=tt[0,:,:]\n",
    "fig=plt.matshow(img,cmap=plt.cm.gray)\n",
    "plt.colorbar(fig)\n",
    "plt.xlabel('Place cells 1')\n",
    "plt.ylabel('State')\n",
    "\n",
    "tt=np.array(place_cells_mus)\n",
    "img=tt[1,:,:]\n",
    "fig=plt.matshow(img,cmap=plt.cm.gray)\n",
    "plt.colorbar(fig)\n",
    "plt.xlabel('Place cells 2')\n",
    "plt.ylabel('State')\n",
    "\n",
    "pp=np.array(grid_cells_mus)\n",
    "plt.matshow(np.concatenate((pp[0:batch_size/2,:],pp[batch_size/2:batch_size,:]),axis=1),cmap=plt.cm.gray)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Grid cells context 1 - Context 2')\n",
    "plt.ylabel('State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
